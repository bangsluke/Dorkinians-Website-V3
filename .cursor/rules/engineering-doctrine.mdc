---
alwaysApply: true
---
# Engineering Doctrine - V3 Dorkinians Website

## Table of Contents

- [Table of Contents](#table-of-contents)
- [React \& Next.js Best Practices](#react--nextjs-best-practices)
  - [Component Lifecycle \& Hooks Safety](#component-lifecycle--hooks-safety)
    - [React Hooks Ordering in Transition Components](#react-hooks-ordering-in-transition-components)
    - [HTML Validation in React Components](#html-validation-in-react-components)
  - [State Management Patterns](#state-management-patterns)
    - [Zustand Store Design](#zustand-store-design)
    - [Async Data Loading Store Pattern](#async-data-loading-store-pattern)
    - [Component State Management](#component-state-management)
  - [Async Data Loading UI State Protocol](#async-data-loading-ui-state-protocol)
  - [Debugging \& Problem Solving](#debugging--problem-solving)
    - [Evidence-Based Debugging Protocol](#evidence-based-debugging-protocol)
  - [Schema Alignment Verification Protocol](#schema-alignment-verification-protocol)
  - [Change Verification Protocol](#change-verification-protocol)
  - [Debug-First Approach Protocol](#debug-first-approach-protocol)
  - [Systematic Error Investigation Protocol](#systematic-error-investigation-protocol)
  - [Naming Consistency Enforcement Protocol](#naming-consistency-enforcement-protocol)
  - [Complete Data Flow Analysis Protocol](#complete-data-flow-analysis-protocol)
  - [Complete Request Flow Verification Protocol](#complete-request-flow-verification-protocol)
  - [Multi-Layer Debugging Protocol](#multi-layer-debugging-protocol)
    - [Progressive Problem Escalation](#progressive-problem-escalation)
- [Build \& Development Workflow](#build--development-workflow)
  - [TypeScript \& Build Safety](#typescript--build-safety)
  - [Component Architecture](#component-architecture)
  - [TypeScript Path Aliases](#typescript-path-aliases)
  - [Schema Synchronization Protocol](#schema-synchronization-protocol)
- [Next.js App Router Patterns](#nextjs-app-router-patterns)
  - [App Router File Structure](#app-router-file-structure)
  - [Route Handlers](#route-handlers)
  - [Metadata and Viewport Configuration](#metadata-and-viewport-configuration)
  - [Server vs Client Components](#server-vs-client-components)
  - [Next.js API Route Patterns](#nextjs-api-route-patterns)
- [PWA Configuration](#pwa-configuration)
  - [PWA Setup with next-pwa](#pwa-setup-with-next-pwa)
  - [Service Worker Caching Strategies](#service-worker-caching-strategies)
  - [PWA Manifest Configuration](#pwa-manifest-configuration)
  - [PWA Update Notifications](#pwa-update-notifications)
  - [Development vs Production PWA Behavior](#development-vs-production-pwa-behavior)
- [Documentation Standards](#documentation-standards)
  - [Markdown Documentation Protocol](#markdown-documentation-protocol)
  - [Documentation Integration Protocol](#documentation-integration-protocol)
- [Quality Gates](#quality-gates)
  - [Pre-Deployment Checklist](#pre-deployment-checklist)
  - [Testing Protocol](#testing-protocol)
  - [Test Script Safety Protocol](#test-script-safety-protocol)
  - [User Experience Validation Protocol](#user-experience-validation-protocol)
  - [Proactive UX Design Protocol](#proactive-ux-design-protocol)
  - [User Experience in Technical Systems Protocol](#user-experience-in-technical-systems-protocol)
  - [Simple UI Implementation Protocol](#simple-ui-implementation-protocol)
  - [Fallback Mechanism Design](#fallback-mechanism-design)
  - [Iterative Correction Protocol](#iterative-correction-protocol)
  - [Documentation Verification Protocol](#documentation-verification-protocol)
- [Testing \& Data Validation](#testing--data-validation)
  - [Jest Configuration with ts-jest](#jest-configuration-with-ts-jest)
  - [Test File Structure](#test-file-structure)
  - [Production Database Testing](#production-database-testing)
  - [Test Utilities and Helpers](#test-utilities-and-helpers)
  - [Test Coverage Configuration](#test-coverage-configuration)
  - [Test Output and Reporting](#test-output-and-reporting)
  - [Data Source Integrity Protocol](#data-source-integrity-protocol)
  - [Data Structure Verification Protocol](#data-structure-verification-protocol)
  - [Test Failure Logic Protocol](#test-failure-logic-protocol)
  - [TBL\_TestData Validation Protocol](#tbl_testdata-validation-protocol)
  - [External Service Connection Protocol](#external-service-connection-protocol)
  - [Structured Error Handling Protocol](#structured-error-handling-protocol)
  - [Evidence-Based Debugging Protocol](#evidence-based-debugging-protocol-1)
  - [Evidence-Based Diagnosis Protocol](#evidence-based-diagnosis-protocol)
  - [Data Pipeline Verification Protocol](#data-pipeline-verification-protocol)
  - [Log-First Debugging Strategy](#log-first-debugging-strategy)
  - [User Instruction Adherence Protocol](#user-instruction-adherence-protocol)
- [Natural Language Processing \& Query Analysis](#natural-language-processing--query-analysis)
  - [Context-Aware Entity Classification Protocol](#context-aware-entity-classification-protocol)
  - [User Domain Knowledge Leverage Protocol](#user-domain-knowledge-leverage-protocol)
  - [Incremental Problem Resolution Protocol](#incremental-problem-resolution-protocol)
  - [Evidence-Based Hypothesis Protocol](#evidence-based-hypothesis-protocol)
  - [Database Schema Verification Protocol](#database-schema-verification-protocol)
  - [Pipeline Order Discipline Protocol](#pipeline-order-discipline-protocol)
  - [Incremental Fix Validation Protocol](#incremental-fix-validation-protocol)
  - [Fuzzy Matching Extension Protocol](#fuzzy-matching-extension-protocol)
  - [User Experience Formatting Protocol](#user-experience-formatting-protocol)
  - [Question Analysis Priority Protocol](#question-analysis-priority-protocol)
  - [Graph Database Relationship Counting Protocol](#graph-database-relationship-counting-protocol)
  - [Systematic Data Discrepancy Analysis Protocol](#systematic-data-discrepancy-analysis-protocol)
  - [Data Corruption Investigation Protocol](#data-corruption-investigation-protocol)
  - [User Technical Guidance Priority Protocol](#user-technical-guidance-priority-protocol)
  - [Evidence-First Problem Solving Protocol](#evidence-first-problem-solving-protocol)
  - [Systematic Problem-Solving Protocol](#systematic-problem-solving-protocol)
  - [Holistic Solution Design Protocol](#holistic-solution-design-protocol)
  - [Recovery System Transparency Protocol](#recovery-system-transparency-protocol)
- [Database Integrity \& Data Management](#database-integrity--data-management)
  - [Data Source Verification Protocol](#data-source-verification-protocol)
  - [User Data Authority Protocol](#user-data-authority-protocol)
  - [Database Relationship Integrity Protocol](#database-relationship-integrity-protocol)
  - [Data Integrity Testing Protocol](#data-integrity-testing-protocol)
  - [Batch Processing Safety Protocol](#batch-processing-safety-protocol)
  - [Memory Management in Constrained Environments Protocol](#memory-management-in-constrained-environments-protocol)
  - [Multi-Layer Data Persistence Protocol](#multi-layer-data-persistence-protocol)
  - [Platform-Aware Persistence Protocol](#platform-aware-persistence-protocol)
  - [Graceful Degradation Protocol](#graceful-degradation-protocol)
  - [Git Hook Auto-Sync Protocol](#git-hook-auto-sync-protocol)
- [Environment-Specific Operations](#environment-specific-operations)
  - [Shell Environment Compatibility Protocol](#shell-environment-compatibility-protocol)
  - [Environment Configuration Protocol](#environment-configuration-protocol)
  - [Progressive Development Protocol](#progressive-development-protocol)
  - [External Data Validation Protocol](#external-data-validation-protocol)
  - [Temporary File Management Protocol](#temporary-file-management-protocol)
  - [Netlify Redirect Configuration Protocol](#netlify-redirect-configuration-protocol)
- [Graph Database Operations](#graph-database-operations)
  - [Graph Data Safety Protocol](#graph-data-safety-protocol)
  - [Database Operations Protocol](#database-operations-protocol)
  - [Schema Design Principles Protocol](#schema-design-principles-protocol)
  - [Query Generation Safety Protocol](#query-generation-safety-protocol)

## React & Next.js Best Practices

### Component Lifecycle & Hooks Safety

#### React Hooks Ordering in Transition Components

- **Rule**: Never place conditional returns after hooks in components that may be rendered during transitions (AnimatePresence, Suspense, etc.)
- **Rationale**: Transition libraries keep components mounted during exit animations, causing hooks ordering violations when conditional returns change the execution path
- **Implementation**: Use parent-level conditional rendering instead of component-level conditional returns for transition components
- **Example**:

  ```typescript
  // ❌ BAD - Conditional return after hooks in AnimatePresence component
  function StatsContainer() {
    const { currentMainPage } = useNavigationStore();
    // ... other hooks
    if (currentMainPage !== "stats") return null; // Causes hooks ordering violation
  }

  // ✅ GOOD - Parent-level conditional rendering
  case "stats":
    return currentMainPage === "stats" ? <StatsContainer /> : null;
  ```

#### HTML Validation in React Components

- **Rule**: Always validate that React component structure produces valid HTML, especially with table elements
- **Rationale**: Invalid HTML structure causes hydration errors and breaks SSR/SSG
- **Implementation**: Use proper React patterns for table elements and avoid wrapping table elements in non-table containers
- **Example**:

  ```typescript
  // ❌ BAD - Invalid HTML structure
  <StatTooltip>
    <tr>...</tr>  // <tr> inside <div> from StatTooltip
  </StatTooltip>

  // ✅ GOOD - Valid HTML structure
  <StatRow stat={stat} value={value} playerData={playerData} />
  // Where StatRow renders <tr> directly in <tbody>
  ```

### State Management Patterns

#### Zustand Store Design

- **Rule**: Design store actions to be atomic and avoid side effects that trigger during render
- **Rationale**: Prevents state updates during render phase which violate React rules
- **Implementation**: Use useEffect for side effects, keep store actions pure

#### Async Data Loading Store Pattern

- **Rule**: Implement background data loading with store-level caching to improve user experience
- **Rationale**: Pre-loading filter data eliminates loading delays and improves perceived performance
- **Implementation**:
  - Load data asynchronously on site initialization using Promise.all for parallel loading
  - Cache data in store state with loading flags to prevent duplicate API calls
  - Use data array length checks for UI state decisions rather than loading flags
- **Example**: Load seasons, teams, opposition, and competitions data in parallel on site load, cache in store, check `filterData.seasons.length === 0` for loading states

#### Component State Management

- **Rule**: Each component should manage its own tooltip/UI state independently
- **Rationale**: Prevents state conflicts and makes components more reusable
- **Implementation**: Use individual useState hooks per component instance rather than shared state

### Async Data Loading UI State Protocol

- **Rule**: When implementing background data loading, always check actual data availability (array length) rather than loading flags for UI state decisions
- **Rationale**: Loading flags may not accurately reflect data availability during async operations, causing false loading states
- **Implementation**: Use `dataArray.length === 0` instead of `!isLoading` for loading state conditions
- **Example**: `{filterData.seasons.length === 0 ? "Loading..." : <DataComponent />}` instead of `{!isLoading ? "Loading..." : <DataComponent />}`

### Debugging & Problem Solving

#### Evidence-Based Debugging Protocol

- **Rule**: Use systematic logging and step-by-step analysis rather than theoretical fixes
- **Rationale**: Console logs provide concrete evidence of the actual execution flow
- **Implementation**: Add strategic console.log statements to trace state changes and component lifecycle

### Schema Alignment Verification Protocol

- **Rule**: When making backend schema changes, proactively check and update frontend code to ensure consistency
- **Rationale**: Schema changes often require corresponding updates to frontend queries and data access patterns
- **Implementation**:
  - Review frontend chatbot queries for outdated relationship types or data sources
  - Update query patterns to match new schema definitions
  - Verify relationship directions and property access patterns
- **Example**: Backend schema changes Captain Awards relationship type → update frontend queries from `CAPTAIN` to `HAS_CAPTAIN_AWARDS`

### Change Verification Protocol

- **Rule**: Always verify that code changes are actually applied and active in the running system before considering fixes complete
- **Rationale**: Development environments with caching, hot reloading, or build systems may not immediately reflect changes, leading to false assumptions about fix effectiveness
- **Implementation**:
  - Test changes immediately after implementation using actual API calls or user scenarios
  - Restart development servers when necessary to ensure changes are loaded
  - Verify behavior matches expected changes, not just that code was modified
- **Example**: Entity extraction changes not working → restart dev server → verify with actual chatbot API calls

### Debug-First Approach Protocol

- **Rule**: Create targeted debug scripts to isolate and verify issues before making modifications to production code
- **Rationale**: Debug scripts provide controlled environments to test hypotheses and verify root causes without affecting the main system
- **Implementation**:
  - Create focused debug scripts that test specific components or patterns
  - Use debug scripts to verify assumptions before implementing fixes
  - Clean up debug scripts after successful problem resolution
- **Example**: Pattern matching issues → create debug script to test regex patterns → verify correct patterns → implement targeted fixes

### Systematic Error Investigation Protocol

- **Rule**: When users report multiple errors, investigate each error individually rather than trying to fix all at once
- **Rationale**: Each error may have a different root cause, and parallel debugging often leads to incomplete fixes and missed root causes
- **Implementation**:
  - Identify and fix the most critical issue first
  - Verify the fix works before moving to the next issue
  - Use systematic verification at each step
  - Create targeted diagnostic scripts for each specific error
- **Example**: Captain Awards + Opposition + Fixture connection errors → investigate Captain Awards first → verify fix → then investigate Opposition → verify fix → then investigate Fixture connections

### Naming Consistency Enforcement Protocol

- **Rule**: Maintain consistent naming conventions across all system components (entity extraction, priority systems, metric mapping) to prevent mismatches
- **Rationale**: Inconsistent naming between system components causes silent failures where data flows correctly but components can't find each other
- **Implementation**:
  - Audit naming conventions across all related components
  - Ensure entity extraction outputs match priority system inputs
  - Verify metric mappings use consistent naming throughout the pipeline
- **Example**: Entity extraction finds "Conceded Per Appearance" but priority system looks for "Goals Conceded Per Appearance" → align naming conventions

### Complete Data Flow Analysis Protocol

- **Rule**: When debugging complex systems, trace the entire data pipeline from input to output, not just suspected problem areas
- **Rationale**: Root causes often exist upstream from where symptoms appear, and fixing downstream issues creates fragile workarounds
- **Implementation**:
  - Map the complete data flow: input → analysis → processing → output
  - Add logging at each stage to trace data transformation
  - Identify where the data diverges from expected behavior
  - Verify each component in the pipeline independently before making changes
- **Example**: Query building issues → trace from question analysis through entity extraction to query generation

### Complete Request Flow Verification Protocol

- **Rule**: When debugging API issues, always trace the complete request flow from frontend through all middleware to backend execution
- **Rationale**: API failures can occur at any stage (frontend → Netlify → Heroku), and fixing the wrong stage wastes time
- **Implementation**:
  - Test each component independently: frontend → Netlify function → Heroku endpoint
  - Use direct API calls to verify backend functionality before investigating frontend integration
  - Check parameter passing between each stage (query params vs body params vs headers)
  - Verify network connectivity and configuration at each layer
  - Add comprehensive logging at every layer to create complete audit trails
- **Example**: "Job ID not found" → test Heroku endpoint directly → verify Netlify function → check frontend parameter passing

### Multi-Layer Debugging Protocol

- **Rule**: Implement systematic logging at every layer of a distributed system to create complete audit trails for debugging
- **Rationale**: Complex systems have multiple failure points, and comprehensive logging enables precise root cause identification
- **Implementation**:
  - Add logging at frontend (request data), middleware (parameter processing), and backend (execution flow)
  - Use consistent log prefixes and structured data for easy filtering and analysis
  - Log both successful operations and failures with sufficient context
  - Include timing information and correlation IDs to trace request flow
- **Example**: Frontend logs request body → Netlify logs parameter parsing → Heroku logs execution steps → Email service logs processing

#### Progressive Problem Escalation

- **Rule**: When initial fixes fail, systematically escalate the approach rather than repeating the same pattern
- **Rationale**: Surface-level fixes often miss root causes
- **Implementation**: Start with symptoms, then move to component structure, then to architectural changes

## Build & Development Workflow

### TypeScript & Build Safety

- **Rule**: Always run type-check and build verification after making changes
- **Rationale**: Catches type errors and build issues before they reach production
- **Implementation**: Use `npm run type-check` and `npm run build` as verification steps

### Component Architecture

- **Rule**: Separate concerns between data fetching, state management, and presentation
- **Rationale**: Makes components more testable and maintainable
- **Implementation**: Use custom hooks for data fetching, Zustand for global state, and pure components for presentation

### TypeScript Path Aliases

- **Rule**: Use TypeScript path aliases configured in tsconfig.json for consistent imports
- **Rationale**: Path aliases improve code readability and make refactoring easier
- **Implementation**:
  - Use `@/` prefix for root-level imports (e.g., `@/components`, `@/lib`, `@/types`)
  - Configure aliases in `tsconfig.json` under `compilerOptions.paths`
  - Ensure Jest configuration includes matching `moduleNameMapper` for test imports
- **Example**: `import { useNavigationStore } from '@/lib/stores/navigation'` instead of relative paths like `../../../lib/stores/navigation`

### Schema Synchronization Protocol

- **Rule**: Maintain schema consistency across repositories using the sync-config script
- **Rationale**: Schema changes must be synchronized between `database-dorkinians` (master) and `V3-Dorkinians-Website` to ensure consistency
- **Implementation**:
  - Master schema location: `database-dorkinians/config/schema.js`
  - Master data sources: `database-dorkinians/config/dataSources.js`
  - Run `npm run sync-config` from V3-Dorkinians-Website to sync schema files
  - Always edit schema in the master location first, then sync
  - Deploy both repositories after schema changes to maintain consistency
- **Example**: Edit `database-dorkinians/config/schema.js` → run `npm run sync-config` in V3-Dorkinians-Website → deploy both repositories

## Next.js App Router Patterns

### App Router File Structure

- **Rule**: Use Next.js 14 App Router with `app/` directory structure for routing and layouts
- **Rationale**: App Router provides better performance, improved layouts, and server components by default
- **Implementation**:
  - Place route handlers in `app/api/*/route.ts` files
  - Use `app/layout.tsx` for root layout with metadata and global providers
  - Use nested `layout.tsx` files for shared layouts (e.g., `app/settings/layout.tsx`)
  - Place page components in `page.tsx` files
  - Use `error.tsx` for error boundaries at route level
- **Example**: `app/api/chatbot/route.ts` for chatbot API, `app/page.tsx` for home page, `app/layout.tsx` for root layout

### Route Handlers

- **Rule**: Export named HTTP method functions (GET, POST, OPTIONS) from route.ts files
- **Rationale**: Next.js App Router uses named exports to determine which HTTP methods are supported
- **Implementation**:
  - Export async functions named after HTTP methods: `export async function GET()`, `export async function POST()`, etc.
  - Use `NextRequest` and `NextResponse` from `next/server` for type safety
  - Handle OPTIONS requests for CORS preflight
  - Return `NextResponse.json()` for JSON responses
- **Example**:
  ```typescript
  import { NextRequest, NextResponse } from 'next/server';
  
  export async function OPTIONS() {
    return new NextResponse(null, { status: 200, headers: corsHeaders });
  }
  
  export async function POST(request: NextRequest) {
    const body = await request.json();
    return NextResponse.json({ data: body }, { headers: corsHeaders });
  }
  ```

### Metadata and Viewport Configuration

- **Rule**: Configure metadata and viewport in layout.tsx using Next.js metadata API
- **Rationale**: Centralized metadata configuration improves SEO and PWA support
- **Implementation**:
  - Export `metadata` object with title, description, manifest, and PWA settings
  - Export `viewport` object for responsive design configuration
  - Use `appleWebApp` metadata for iOS PWA support
  - Include startup images for different device sizes
- **Example**:
  ```typescript
  export const metadata: Metadata = {
    title: "Dorkinians FC Stats",
    description: "Dorkinians FC Statistics and Chatbot - Mobile-first PWA",
    manifest: "/manifest.json",
    appleWebApp: { capable: true, statusBarStyle: "default" }
  };
  
  export const viewport = {
    width: "device-width",
    initialScale: 1,
    maximumScale: 5
  };
  ```

### Server vs Client Components

- **Rule**: Use Server Components by default, mark Client Components with "use client" directive only when needed
- **Rationale**: Server Components reduce bundle size and improve performance by running on the server
- **Implementation**:
  - Default to Server Components (no directive needed)
  - Add `"use client"` directive only for components using hooks, browser APIs, or event handlers
  - Keep data fetching in Server Components when possible
  - Use Client Components for interactive UI elements
- **Example**: API routes and layouts are Server Components by default; components with useState/useEffect need "use client"

### Next.js API Route Patterns

- **Rule**: Implement consistent CORS handling, error responses, and request validation in API routes
- **Rationale**: Standardized API patterns improve maintainability and security
- **Implementation**:
  - Define `corsHeaders` object with CORS configuration for reuse
  - Always handle OPTIONS requests for CORS preflight
  - Validate request body and parameters before processing
  - Return structured error responses with appropriate status codes
  - Use try-catch blocks with user-friendly error messages
  - Log errors server-side but return sanitized messages to clients
- **Example**:
  ```typescript
  const corsHeaders = {
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
    "Access-Control-Allow-Headers": "Content-Type, Authorization",
  };
  
  export async function POST(request: NextRequest) {
    try {
      const body = await request.json();
      if (!body.question) {
        return NextResponse.json(
          { error: "Question is required" },
          { status: 400, headers: corsHeaders }
        );
      }
      // Process request...
      return NextResponse.json(response, { headers: corsHeaders });
    } catch (error) {
      console.error("API error:", error);
      return NextResponse.json(
        { error: "Internal server error" },
        { status: 500, headers: corsHeaders }
      );
    }
  }
  ```

## PWA Configuration

### PWA Setup with next-pwa

- **Rule**: Configure PWA using next-pwa wrapper in next.config.js with appropriate caching strategies
- **Rationale**: PWA configuration enables offline functionality and improved mobile experience
- **Implementation**:
  - Wrap Next.js config with `withPWA()` from `next-pwa`
  - Disable PWA in development (`disable: process.env.NODE_ENV === "development"`)
  - Configure runtime caching strategies for different asset types
  - Set `skipWaiting: false` to ensure users get updates
  - Output service worker to `public` directory
- **Example**:
  ```javascript
  const withPWA = require("next-pwa")({
    dest: "public",
    register: true,
    skipWaiting: false,
    disable: process.env.NODE_ENV === "development",
    runtimeCaching: [/* caching strategies */]
  });
  ```

### Service Worker Caching Strategies

- **Rule**: Use appropriate caching strategies for different asset types to balance performance and freshness
- **Rationale**: Different assets have different update frequencies and importance
- **Implementation**:
  - Use `CacheFirst` for static assets (fonts, images) with long expiration
  - Use `StaleWhileRevalidate` for assets that can be updated in background
  - Configure `maxEntries` and `maxAgeSeconds` based on asset type
  - Cache fonts for 1 year, images for 30 days
- **Example**:
  ```javascript
  {
    urlPattern: /^https:\/\/fonts\.(googleapis|gstatic)\.com\/.*/i,
    handler: "CacheFirst",
    options: {
      cacheName: "google-fonts",
      expiration: { maxEntries: 4, maxAgeSeconds: 60 * 60 * 24 * 365 }
    }
  }
  ```

### PWA Manifest Configuration

- **Rule**: Configure comprehensive PWA manifest with all required icons and metadata
- **Rationale**: Complete manifest ensures proper PWA installation and display across platforms
- **Implementation**:
  - Include manifest.json in public directory with name, short_name, icons, theme_color
  - Reference manifest in layout.tsx metadata
  - Provide icons in multiple sizes (16x16 to 512x512)
  - Configure appleWebApp metadata for iOS support
  - Include startup images for different iOS device sizes
- **Example**: Reference `/manifest.json` in metadata, provide icons in `public/icons/` directory

### PWA Update Notifications

- **Rule**: Implement PWA update notification system to inform users of new versions
- **Rationale**: Users need to be notified when new service worker versions are available
- **Implementation**:
  - Listen for `controllerchange` event to detect service worker updates
  - Show update notification UI when new version is available
  - Provide "Update" button to skip waiting and activate new service worker
  - Use `skipWaiting()` and `clients.claim()` to activate updates
- **Example**: `PWAUpdateNotification` component listens for updates and shows notification banner

### Development vs Production PWA Behavior

- **Rule**: Disable PWA in development to avoid service worker caching issues during development
- **Rationale**: Service worker caching can interfere with hot reloading and make debugging difficult
- **Implementation**:
  - Set `disable: process.env.NODE_ENV === "development"` in next-pwa config
  - PWA features (service worker, offline support) only active in production builds
  - Test PWA functionality in production builds, not development mode
- **Example**: Service worker not registered in development, only in production builds

## Documentation Standards

### Markdown Documentation Protocol

- **Rule**: When creating or updating documentation, always follow the user's established formatting standards from the beginning
- **Rationale**: Prevents the need for user corrections and ensures consistency across all documentation
- **Implementation**:
  - Include comprehensive Table of Contents with all subsections (H2, H3, H4) and proper anchor links
  - Add "Back to Table of Contents" navigation links between all major sections
  - Follow the user's specific markdown structure requirements consistently

### Documentation Integration Protocol

- **Rule**: When merging content from multiple sources, ensure all structural elements are comprehensive and match established standards
- **Rationale**: Prevents incomplete integration that requires user enhancement
- **Implementation**:
  - Anticipate the full depth of documentation structure needed
  - Include all subsections in table of contents, not just major sections
  - Verify complete integration before considering the task complete

## Quality Gates

### Pre-Deployment Checklist

1. TypeScript compilation passes (`npm run type-check`)
2. Build completes successfully (`npm run build`)
3. No React hooks ordering violations
4. No HTML validation errors
5. No hydration mismatches
6. All user-reported issues resolved
7. Documentation follows established formatting standards
8. User experience validation completed (actual user-visible results confirmed)
9. Fallback mechanisms tested and functional
10. User corrections addressed with comprehensive solutions

### Testing Protocol

1. Test the specific failing scenario reported by user
2. Test related functionality to ensure no regressions
3. Verify console logs show expected behavior
4. Confirm no new errors introduced

### Test Script Safety Protocol

- **Rule**: All test scripts that create database connections or long-running processes must include timeout and proper exit handling
- **Rationale**: Scripts that hang indefinitely create poor user experience and require manual intervention to terminate
- **Implementation**:
  - Add timeout mechanisms (30-60 seconds) for long-running operations
  - Implement proper process exit handling (SIGINT, SIGTERM)
  - Clear timeouts and exit cleanly on completion or error
  - Add progress indicators for long-running operations
- **Example**: Database connection scripts → add `setTimeout(() => process.exit(1), 30000)` and `process.on('SIGINT', () => process.exit(0))`

### User Experience Validation Protocol

- **Rule**: Technical execution success must be validated against actual user experience, not just system metrics
- **Rationale**: Scripts can run successfully while failing to deliver the expected user-visible results
- **Implementation**:
  - For email reports: Verify actual email content structure, not just delivery success
  - For UI changes: Confirm user-visible changes, not just component rendering
  - For data generation: Distinguish between sample data and comprehensive coverage requirements
- **Example**: Email script reports "sent successfully" but user reports "table not showing" - investigate actual email content

### Proactive UX Design Protocol

- **Rule**: Consider user experience and space efficiency during initial implementation, not just after user feedback
- **Rationale**: Proactive UX design prevents user corrections and improves initial user satisfaction
- **Implementation**:
  - Use grid layouts for checkbox lists to improve space efficiency
  - Consider mobile responsiveness and screen real estate during design
  - Anticipate user workflow needs and optimize accordingly
- **Example**: Implement two-column layout for seasons checkboxes during initial development rather than waiting for user feedback about space efficiency

### User Experience in Technical Systems Protocol

- **Rule**: Balance technical completeness with user experience - verbose logging may be technically correct but poor UX
- **Rationale**: Technical systems must serve users effectively, not just function correctly
- **Implementation**:
  - Consolidate verbose logs into single, informative lines
  - Prioritize user-visible feedback over technical completeness
  - Implement clean, consistent UI patterns across all components
  - Use systematic approaches to reduce visual clutter and improve readability
- **Example**: Multiple debug logs → consolidate into single comprehensive log with visual progress indicators

### Simple UI Implementation Protocol

- **Rule**: When users request simple UI changes, implement them directly without over-engineering complex solutions
- **Rationale**: Over-engineering simple requests creates unnecessary complexity and delays delivery
- **Implementation**:
  - Implement user requests as directly as possible
  - Avoid creating complex systems (tabs, modals, etc.) when simple solutions suffice
  - Focus on the core requirement rather than adding "enhancements"
- **Example**: User wants monitoring section moved → move it directly rather than creating a tab system

### Fallback Mechanism Design

- **Rule**: For unreliable parsing operations, implement robust fallback strategies that ensure core functionality
- **Rationale**: Complex output parsing (Jest verbose, API responses) is inherently fragile and can fail silently
- **Implementation**:
  - Always provide comprehensive fallback data generation when parsing fails
  - Design fallbacks to match the full scope of expected results, not just sample data
  - Log fallback activation clearly for debugging purposes
- **Example**: Jest output parsing fails → generate comprehensive test details covering all scenarios and players

### Iterative Correction Protocol

- **Rule**: Treat user corrections as critical failure signals requiring immediate investigation and course correction
- **Rationale**: User feedback indicates the actual success criteria, not technical metrics
- **Implementation**:
  - When user corrects "still not working", immediately investigate the gap between technical success and user experience
  - Don't repeat the same approach that failed; escalate to more comprehensive solutions
  - Validate fixes against user confirmation, not just technical execution

### Documentation Verification Protocol

1. Table of Contents includes all subsections with proper anchor links
2. Navigation links are present between all major sections
3. Formatting matches user's established standards
4. No redundant or duplicate documentation files remain

## Testing & Data Validation

### Jest Configuration with ts-jest

- **Rule**: Use Jest with ts-jest preset for TypeScript testing with proper configuration
- **Rationale**: ts-jest enables TypeScript compilation in Jest tests and provides type checking
- **Implementation**:
  - Configure Jest with `preset: "ts-jest"` in jest.config.js
  - Use `tsconfig.test.json` for test-specific TypeScript configuration
  - Configure `moduleNameMapper` to match tsconfig path aliases (`@/` imports)
  - Set `testEnvironment: "node"` for API and service tests
  - Configure `testTimeout: 60000` for database operations that may take longer
  - Use `forceExit: true` to ensure tests complete cleanly
- **Example**:
  ```javascript
  module.exports = {
    preset: "ts-jest",
    testEnvironment: "node",
    moduleNameMapper: { "^@/(.*)$": "<rootDir>/$1" },
    testTimeout: 60000,
    forceExit: true
  };
  ```

### Test File Structure

- **Rule**: Organize tests in `__tests__` directory with clear categorization
- **Rationale**: Organized test structure improves maintainability and makes test discovery easier
- **Implementation**:
  - Place test files in `__tests__/` directory at root level
  - Organize by category: `services/`, `validation/`, `ux/`, `security/`, etc.
  - Use descriptive test file names: `chatbotService.test.ts`, `dataAccuracyValidation.test.ts`
  - Match test file patterns: `**/__tests__/**/*.ts` and `**/?(*.)+(spec|test).ts`
- **Example**: `__tests__/services/chatbotService.test.ts`, `__tests__/validation/dataAccuracyValidation.test.ts`

### Production Database Testing

- **Rule**: Configure tests to connect to production database for real-world validation
- **Rationale**: Testing against production database ensures tests validate actual system behavior and data accuracy
- **Implementation**:
  - Use production database credentials in test environment
  - Set longer test timeout (60s) for database operations
  - Implement proper connection cleanup in test teardown
  - Use `clearMocks`, `resetMocks`, `restoreMocks` to ensure test isolation
  - Test actual Neo4j queries and response times
- **Example**: Tests connect to Neo4j Aura production database to validate real chatbot performance

### Test Utilities and Helpers

- **Rule**: Create reusable test utilities and helpers for common testing patterns
- **Rationale**: Test utilities reduce duplication and improve test consistency
- **Implementation**:
  - Create `__tests__/utils/testUtils.ts` for shared test helpers
  - Implement data fetching utilities for test data (CSV, API)
  - Create mock data generators for consistent test data
  - Implement assertion helpers for common validation patterns
- **Example**: `testUtils.ts` provides CSV loading, data validation, and assertion helpers

### Test Coverage Configuration

- **Rule**: Configure test coverage collection for key directories with appropriate exclusions
- **Rationale**: Coverage metrics help identify untested code and guide testing efforts
- **Implementation**:
  - Configure `collectCoverageFrom` to include `lib/**/*.ts` and `components/**/*.tsx`
  - Exclude type definition files (`!**/*.d.ts`) and node_modules
  - Use multiple coverage reporters: `["text", "lcov", "html"]`
  - Set coverage directory for output files
- **Example**: Coverage collected from lib and components, excluding type definitions

### Test Output and Reporting

- **Rule**: Configure clean test output with optional verbose mode for debugging
- **Rationale**: Clean output improves readability, while verbose mode helps with debugging
- **Implementation**:
  - Use custom summary reporter for clean default output
  - Enable verbose mode via `JEST_VERBOSE=true` environment variable
  - Configure reporters conditionally based on verbose flag
  - Use `jest.setup.js` for global test configuration
- **Example**: Default clean output, verbose mode with `npm run test:debug` shows detailed logs

### Data Source Integrity Protocol

- **Rule**: Never implement hardcoded fallback values when the user explicitly requires dynamic data sourcing
- **Rationale**: Hardcoded values violate the fundamental requirement for real data validation and can mask data access issues
- **Implementation**:
  - Always find a way to access the real data source, even if it requires alternative approaches
  - When TypeScript imports fail, implement direct HTTP fetching to CSV/API sources
  - If data access fails, report the failure clearly rather than using placeholder data
- **Example**: CSV import fails → implement direct HTTP fetch to Google Sheets CSV URL

### Data Structure Verification Protocol

- **Rule**: Before making assumptions about data field names or structure, always inspect the actual data first
- **Rationale**: Field names in external data sources may not match JavaScript naming conventions
- **Implementation**:
  - Add strategic logging to examine actual data structure
  - Use flexible field access patterns (case-insensitive, multiple naming conventions)
  - Verify field names match exactly what's in the source data
- **Example**: CSV headers are `'PLAYER NAME'` not `playerName` or `PlayerName`

### Test Failure Logic Protocol

- **Rule**: Tests with missing, null, undefined, or "N/A" values must be marked as FAILED, not PASSED
- **Rationale**: Missing data indicates a problem that should be flagged, not ignored
- **Implementation**:
  - Implement strict validation functions that mark invalid data as failed
  - Never allow tests to pass when expected data is unavailable
  - Distinguish between "no data available" (FAILED) and "data available but zero" (PASSED)
- **Example**: `getValueOrFail(value) => value === 'N/A' ? {status: 'FAILED'} : {status: 'PASSED'}`

### TBL_TestData Validation Protocol

- **Rule**: All test data must be sourced from the actual TBL_TestData CSV, with no hardcoded values allowed in the testing setup
- **Rationale**: Ensures test results reflect real data accuracy and prevents false positives from placeholder values
- **Implementation**:
  - Source all expected values from the authoritative CSV data
  - Implement robust CSV parsing that handles the actual data structure
  - Generate comprehensive test coverage using real data fields
- **Example**: Use `playerData.APP` from CSV rather than hardcoded `172` for Luke Bangs appearances

### External Service Connection Protocol

- **Rule**: Always verify connection state before executing operations on external services (databases, APIs, etc.)
- **Rationale**: External service failures cause cascading errors that are difficult to debug without proper connection verification
- **Implementation**:
  - Add connection checks at the entry point of service operations
  - Return structured error responses when connections fail
  - Log connection attempts and failures for debugging
- **Example**: Neo4j queries fail with "driver not initialized" → add `await neo4jService.connect()` before query execution

### Structured Error Handling Protocol

- **Rule**: Return structured error objects with context rather than `null` or generic failures
- **Rationale**: Structured errors enable proper error handling downstream and provide debugging context
- **Implementation**:
  - Return error objects with `type: "error"`, `error: message`, and `cypherQuery: "N/A"`
  - Handle error types explicitly in response generation
  - Preserve error context through the call chain
- **Example**: `queryPlayerData` returns `{type: "error", data: [], error: "Connection failed"}` instead of `null`

### Evidence-Based Debugging Protocol

- **Rule**: Use systematic analysis (logs, targeted searches) before creating debugging tools
- **Rationale**: Log analysis reveals patterns and root causes more efficiently than creating multiple debug scripts
- **Implementation**:
  - Analyze existing logs and error patterns first
  - Use grep/search tools to identify specific failure points
  - Create targeted fixes based on evidence rather than assumptions
  - Prioritize actual system outputs over theoretical analysis
- **Example**: Analyze test failure logs to identify "No Cypher Query" pattern before creating debug scripts

### Evidence-Based Diagnosis Protocol

- **Rule**: Use actual system outputs rather than theoretical analysis to guide problem-solving
- **Rationale**: Console logs and system outputs provide concrete evidence of actual execution flow
- **Implementation**:
  - Let actual log outputs guide diagnosis rather than making assumptions
  - Use concrete evidence to identify where the process breaks down
  - Test hypotheses with real data before implementing solutions
  - Trace through actual execution flow rather than theoretical behavior
- **Example**: User provides specific log evidence → use that evidence to guide diagnosis rather than theoretical analysis

### Data Pipeline Verification Protocol

- **Rule**: Before fixing data processing issues, always verify the complete pipeline from source to usage
- **Rationale**: Data processing failures can occur at any stage (parsing, access, transformation, usage) and fixing the wrong stage wastes time
- **Implementation**:
  - Trace data flow: source → parsing → access → usage
  - Verify each stage independently before implementing fixes
  - Use concrete data inspection rather than assumptions about data structure
- **Example**: CSV parsing issue → verify actual CSV headers, then parsing logic, then data access patterns

### Log-First Debugging Strategy

- **Rule**: Implement comprehensive logging before attempting fixes, then analyze logs to identify root causes
- **Rationale**: Logs provide concrete evidence of actual execution flow, enabling precise problem identification
- **Implementation**:
  - Add detailed logging at each stage of data processing
  - Analyze log outputs to identify where the process breaks down
  - Use log evidence to guide targeted fixes rather than theoretical solutions
- **Example**: Add CSV parsing logs, data access logs, and test execution logs to trace the complete flow

### User Instruction Adherence Protocol

- **Rule**: When users provide specific debugging methodologies, follow them exactly rather than implementing alternative approaches
- **Rationale**: Users often have domain knowledge about the most effective debugging approaches for their specific systems
- **Implementation**:
  - Follow user-specified debugging steps precisely
  - Implement user-requested logging and analysis approaches
  - Avoid substituting alternative methods without user approval
- **Example**: User requests "use the generated log files to debug" → implement comprehensive logging and analyze logs rather than creating new debug scripts

## Natural Language Processing & Query Analysis

### Context-Aware Entity Classification Protocol

- **Rule**: Distinguish between "core entities" (the "who/what" being queried) and "contextual modifiers" (the "when/where/how" conditions) when assessing query complexity
- **Rationale**: Locations, timeframes, and stat types are filters/conditions, not separate entities that should count toward complexity limits
- **Implementation**:
  - Count only named entities (players, teams, oppositions, leagues) for complexity assessment
  - Treat locations ("home", "away"), timeframes (date ranges), and stat types as contextual filters
  - Only trigger clarification when single entity type exceeds limits (e.g., 4+ teams), not mixed entity types
- **Example**: "How many goals has Luk Bangs got for the 3s whilst playing at home between 20/03/2022 and 21/10/24?" = 2 named entities (1 player + 1 team) + contextual filters (location + timeframe)

### User Domain Knowledge Leverage Protocol

- **Rule**: When users provide domain-specific solutions or corrections, implement them directly rather than over-engineering alternatives
- **Rationale**: Users often have deep understanding of their domain requirements and optimal approaches
- **Implementation**:
  - Prioritize user-suggested technical approaches over complex workarounds
  - Implement user corrections immediately without defending previous decisions
  - Trust user expertise in their specific domain context
- **Example**: User corrects complexity logic → implement user's specific approach rather than creating alternative complexity assessment methods

### Incremental Problem Resolution Protocol

- **Rule**: Fix one issue completely before addressing the next - avoid parallel debugging of multiple problems
- **Rationale**: Parallel debugging often leads to incomplete fixes and missed root causes
- **Implementation**:
  - Identify and fix the most critical issue first
  - Verify the fix works before moving to the next issue
  - Use systematic verification at each step
- **Example**: Fix team detection → verify → fix question type priority → verify → fix duplicate entities → verify

### Evidence-Based Hypothesis Protocol

- **Rule**: Verify assumptions through concrete data before implementation - avoid assumption-driven solutions
- **Rationale**: Assumptions about system behavior often lead to incorrect fixes
- **Implementation**:
  - Use console logs, debug output, and code analysis to verify assumptions
  - Test hypotheses with concrete data before implementing solutions
  - Trace through actual execution flow rather than theoretical behavior
- **Example**: Assume entity counting issue → verify with actual entity extraction logs → implement targeted fix based on evidence

### Database Schema Verification Protocol

- **Rule**: Always verify database schema assumptions through actual queries before implementing business logic
- **Rationale**: Schema assumptions often lead to incorrect query construction and failed operations
- **Implementation**:
  - Run diagnostic queries to confirm data structure before building production logic
  - Verify node relationships and property locations through actual database calls
  - Test query patterns with real data before implementing complex business logic
- **Example**: Assume MatchDetail contains team/location properties → verify schema shows they're in linked Fixture nodes → update queries accordingly

### Pipeline Order Discipline Protocol

- **Rule**: In multi-stage processing systems, ensure classification happens before enhancement/fuzzy matching to prevent incorrect entity typing
- **Rationale**: Entity classification errors cascade through the entire pipeline, making them difficult to correct downstream
- **Implementation**:
  - Implement entity filtering before fuzzy matching to prevent stat types from being classified as players
  - Use similarity-based filtering to exclude recognized entity types from other classification stages
  - Ensure each processing stage has access to results from previous stages
- **Example**: Stat type words ("assits") being classified as players → implement stat type filtering before player extraction

### Incremental Fix Validation Protocol

- **Rule**: Test each change immediately to prevent compound errors and enable clear attribution of what works
- **Rationale**: Multiple simultaneous changes make it impossible to identify which fix resolved the issue
- **Implementation**:
  - Apply one fix at a time and test immediately
  - Use actual API calls and user scenarios for validation
  - Verify each fix works before proceeding to the next issue
- **Example**: Fix team mapping → test → fix time range → test → fix query routing → test

### Fuzzy Matching Extension Protocol

- **Rule**: When adding new entity types to existing fuzzy matching systems, extend the resolver interface rather than creating parallel systems
- **Rationale**: Reusing established patterns ensures consistency and reduces maintenance overhead
- **Implementation**:
  - Extend existing entity resolver interfaces to support new entity types
  - Implement similarity-based matching using established algorithms (Jaro-Winkler, Levenshtein)
  - Maintain consistent confidence thresholds and matching patterns across all entity types
- **Example**: Add stat type fuzzy matching → extend EntityNameResolver interface → implement similarity calculation for stat types

### User Experience Formatting Protocol

- **Rule**: Convert internal data formats to user-friendly formats in responses while maintaining internal consistency
- **Rationale**: Users expect familiar formats (DD/MM/YYYY) rather than technical formats (YYYY-MM-DD)
- **Implementation**:
  - Implement format conversion functions for display purposes
  - Maintain internal consistency with database/API formats
  - Apply formatting at the response generation stage, not data storage stage
- **Example**: Internal dates stored as YYYY-MM-DD → convert to DD/MM/YYYY for user responses

### Question Analysis Priority Protocol

- **Rule**: In chatbot systems, question analysis/entity extraction is often the root cause of query issues, not the query building logic itself
- **Rationale**: Incorrect entity extraction cascades through the entire system, causing downstream components to work with wrong data
- **Implementation**:
  - When query issues occur, first verify question analysis results
  - Check entity extraction accuracy before debugging query construction
  - Trace metric detection from question analysis through to query building
- **Example**: Penalty queries generating home game queries → check if "penalties scored" is being extracted as "HOME" instead of "PSC"

### Graph Database Relationship Counting Protocol

- **Rule**: When counting through relationships in graph databases, use `DISTINCT` to prevent duplicate counting from multiple relationship paths
- **Rationale**: Graph databases can have multiple relationship types between the same nodes, and counting without `DISTINCT` can inflate results by counting the same entity multiple times
- **Implementation**:
  - Use `count(DISTINCT node)` instead of `count(node)` when counting through relationships
  - Identify the specific relationship type that represents the intended count (e.g., `PLAYED_IN` for player appearances)
  - Verify that relationship conditions in schema are specific enough to prevent duplicate relationship creation
- **Example**: Player-MatchDetail relationships → use `count(DISTINCT md)` to count unique match details, not all relationship instances

### Systematic Data Discrepancy Analysis Protocol

- **Rule**: When data counts show consistent mathematical patterns (multipliers, ratios), investigate structural issues rather than logic errors
- **Rationale**: Systematic patterns in data discrepancies often indicate relationship duplication, schema issues, or counting methodology mismatches rather than query logic problems
- **Implementation**:
  - Look for consistent multipliers (e.g., 5-6x higher than expected) as indicators of structural issues
  - Trace the complete data pipeline from input through storage to query execution
  - Compare expected vs actual counts across multiple related metrics to identify patterns
- **Example**: Home games 5x higher than expected → investigate if `HAS_MATCH_DETAILS` relationships are creating duplicates

### Data Corruption Investigation Protocol

- **Rule**: When users report data discrepancies, always verify the actual database state against source data before assuming logic errors
- **Rationale**: Data corruption at the node level can cause relationship queries to return incorrect results, even when the relationship creation logic is correct
- **Implementation**:
  - Create targeted diagnostic scripts to check actual database values vs expected source data
  - Use direct database queries to inspect node properties rather than assuming relationship logic is the issue
  - Investigate data corruption possibilities when unexpected values appear in database queries
- **Example**: Captain Awards showing unexpected player names → check actual CaptainsAndAwards node properties vs CSV source data

### User Technical Guidance Priority Protocol

- **Rule**: When domain experts provide specific technical insights about system behavior, prioritize those insights over theoretical analysis
- **Rationale**: Domain experts have deep understanding of the system's intended behavior and can quickly identify the correct approach
- **Implementation**:
  - When users specify exact technical requirements (e.g., "only count PLAYED_IN relationships"), implement those requirements first
  - Use theoretical analysis to validate and extend user guidance, not replace it
  - Test user-specified approaches before exploring alternative solutions
- **Example**: User says "only count PLAYED_IN relationships" → implement that specific counting method rather than investigating all relationship types

### Evidence-First Problem Solving Protocol

- **Rule**: Always verify assumptions with actual system behavior before proposing solutions
- **Rationale**: Assumptions about system behavior often lead to incorrect fixes and wasted effort
- **Implementation**:
  - Test production systems directly rather than making assumptions about data structure or behavior
  - Use actual CSV data analysis and system testing to identify root causes
  - Let concrete evidence guide diagnosis rather than theoretical analysis
  - When user observations conflict with analysis, investigate the user's perspective first
- **Example**: CSV data analysis reveals empty rows at end of dataset → investigate actual CSV structure rather than assuming data completeness

### Systematic Problem-Solving Protocol

- **Rule**: Follow the pattern: analyze actual data → identify root causes → implement multiple fixes → verify results
- **Rationale**: Systematic approaches prevent incomplete solutions and ensure comprehensive problem resolution
- **Implementation**:
  - Always analyze actual failure data before setting thresholds or limits
  - Identify multiple potential root causes and address each systematically
  - Implement multiple layers of error handling for critical systems
  - Verify each fix independently before moving to the next issue
- **Example**: Memory crash analysis → examine actual crash data → identify multiple causes (thresholds, persistence, recovery) → implement comprehensive solution

### Holistic Solution Design Protocol

- **Rule**: When addressing complex systems, consider all related components and data flows, not just the immediate problem
- **Rationale**: Complex systems have interconnected components, and fixing only the immediate issue often misses related problems
- **Implementation**:
  - Design complete solutions that address multiple related issues simultaneously
  - Consider all related metrics (duration, counts, breakdowns) when combining job results
  - Think about transparency and user experience, not just technical functionality
  - Design for complete visibility into what happened and why
- **Example**: Memory crash recovery → address checkpointing, recovery, email reporting, and data combination as integrated solution

### Recovery System Transparency Protocol

- **Rule**: Recovery mechanisms must provide complete visibility into what happened, why, and what was combined
- **Rationale**: Users need to understand the complete process, especially when recovery was needed
- **Implementation**:
  - Combine data from all job phases (original + recovery) in final reports
  - Clearly indicate when recovery was needed and why
  - Provide breakdown of timing and contributions from each phase
  - Use helper functions that intelligently merge breakdown data by type
- **Example**: Recovery emails show total duration (original + auto-resume), combined node counts, and explicit recovery indicators

## Database Integrity & Data Management

### Data Source Verification Protocol

- **Rule**: When investigating data discrepancies, always confirm which data source the user is referencing before assuming database issues
- **Rationale**: Data synchronization issues between sources can cause apparent discrepancies that aren't actually database problems
- **Implementation**:
  - Ask users to specify which data source they're viewing when reporting discrepancies
  - Verify data source consistency before investigating database issues
  - Accept user corrections about data accuracy immediately rather than continuing to investigate
- **Example**: Database shows 27 players, user sees 14 → confirm user's data source before investigating database corruption

### User Data Authority Protocol

- **Rule**: When users correct data assumptions or report data fixes, accept their corrections immediately without further investigation
- **Rationale**: Users have authoritative knowledge of their data and corrections indicate the actual state of the system
- **Implementation**:
  - Accept user data corrections as definitive
  - Update analysis based on user corrections rather than defending previous assumptions
  - Proceed with fixes based on corrected data state
- **Example**: User reports "data was incorrect, I have now fixed it" → accept correction and re-run integrity tests

### Database Relationship Integrity Protocol

- **Rule**: When creating relationships between database nodes, ensure proper matching logic to prevent incorrect connections
- **Rationale**: Incorrect relationship creation (e.g., connecting all nodes of one type to all nodes of another type) causes massive data corruption
- **Implementation**:
  - Implement specific matching criteria for relationship creation (e.g., opposition team + home/away status)
  - Verify relationship creation logic with small test datasets before full deployment
  - Monitor relationship counts for unexpected inflation patterns
- **Example**: MatchDetail-Fixture relationships → match by opposition team and home/away status, not just date

### Data Integrity Testing Protocol

- **Rule**: Implement comprehensive data integrity tests to catch relationship corruption and data quality issues early
- **Rationale**: Data integrity issues can cause cascading failures throughout the system and are expensive to fix after deployment
- **Implementation**:
  - Create tests for maximum relationships per node, duplicate prevention, and orphaned records
  - Run integrity tests after any major data operations
  - Include tests for business rules (e.g., maximum players per match, maximum fixtures per date)
- **Example**: Test for max 18 MatchDetail nodes per Fixture, max 9 Fixtures per date, no orphaned records

### Batch Processing Safety Protocol

- **Rule**: Use smaller batch sizes and delays for large database operations to prevent timeouts and connection issues
- **Rationale**: Large batch operations can overwhelm database connections and cause operation failures
- **Implementation**:
  - Use batch sizes of 50-100 for complex operations, 1000+ for simple operations
  - Add delays between batches (200-500ms) to prevent overwhelming the database
  - Implement custom count queries for operations that can't use generic batching
- **Example**: PLAYED_IN relationship creation → use 50-record batches with 200ms delays

### Memory Management in Constrained Environments Protocol

- **Rule**: Always set memory thresholds based on actual failure data, not theoretical limits. Implement multiple warning levels with progressive actions.
- **Rationale**: Memory crashes in constrained environments (like Heroku's 512MB limit) require proactive management based on real failure patterns
- **Implementation**:
  - Analyze actual crash data to determine realistic memory thresholds
  - Implement multiple memory warning levels (150MB, 180MB, 200MB) with progressive actions
  - Use aggressive garbage collection triggers and memory-based pauses
  - Implement emergency checkpoint saving before memory limits are reached
- **Example**: Heroku memory crashes at 251MB heap → set thresholds at 200MB heap with emergency checkpoint saving

### Multi-Layer Data Persistence Protocol

- **Rule**: Implement redundant storage mechanisms for critical operational data that cannot be lost
- **Rationale**: Single points of failure in data persistence can cause complete loss of operational state
- **Implementation**:
  - Use multiple storage layers: in-memory, file system, and external storage
  - Implement immediate verification of data persistence after each save operation
  - Design recovery mechanisms that can restore state from any available storage layer
  - Add comprehensive monitoring and debugging capabilities from the start
- **Example**: Job status tracking → in-memory Map + file system + environment variables + real-time monitoring

### Platform-Aware Persistence Protocol

- **Rule**: When designing persistence layers, always consider the target platform's constraints (ephemeral filesystems, memory limits, etc.)
- **Rationale**: Platform constraints can cause data loss if not properly accounted for in persistence design
- **Implementation**:
  - Use database-backed checkpoints as primary storage with file system as fallback for maximum persistence
  - Avoid ephemeral storage locations (like `/tmp` on Heroku) for critical operational data
  - Implement platform-specific storage strategies based on deployment environment
- **Example**: Heroku ephemeral filesystem → use database checkpoints as primary, file system as backup

### Graceful Degradation Protocol

- **Rule**: Non-critical service failures should not prevent critical operations from proceeding
- **Rationale**: Single points of failure in non-essential services can block entire workflows
- **Implementation**:
  - Wrap all external service calls in try-catch blocks to prevent cascade failures
  - Implement fallback mechanisms for non-critical services (email, logging, monitoring)
  - Design critical operations to continue even when supporting services fail
  - Log service failures clearly but don't let them block primary functionality
  - **Example**: Email service failure → log warning but continue with Heroku seeding operation

### Git Hook Auto-Sync Protocol

- **Rule**: Automatically sync Engineering Doctrine to Cursor rules on git commit when the doctrine file changes
- **Rationale**: Eliminates manual sync requirements and ensures Cursor rules are always current
- **Implementation**:
  - Use git pre-commit hook to detect changes to `docs/ENGINEERING_DOCTRINE.md`
  - Automatically run sync script when doctrine changes are committed
  - Fail commit if sync fails to prevent inconsistent state
  - **Example**: Commit changes to doctrine → hook detects change → auto-syncs to `.cursor/rules/engineering-doctrine.mdc`
  - **Status**: ✅ Fully operational and tested

## Environment-Specific Operations

### Shell Environment Compatibility Protocol

- **Rule**: Always verify shell environment and use appropriate command syntax for the target system
- **Rationale**: Different shells (PowerShell, Bash, Zsh) have different syntax requirements and command chaining operators
- **Implementation**:
  - Check shell type before using command chaining operators (`&&` vs `;` vs separate commands)
  - Use PowerShell-compatible commands when working in Windows PowerShell
  - Test command syntax in the target environment before execution
- **Example**: PowerShell doesn't support `&&` → use separate `cd` and `node` commands instead of `cd && node`

### Environment Configuration Protocol

- **Rule**: Always confirm user's preferred environment file structure before implementation
- **Rationale**: Environment configuration preferences vary and incorrect assumptions cause implementation conflicts
- **Implementation**:
  - Prefer single `.env` files with environment-specific prefixes (e.g., `DEV_`, `PROD_`) when user specifies this preference
  - Avoid assumptions about `.env.local` vs `.env` approaches without user confirmation
  - After asking once, assume that an `.env` file exists and only mention it when new values need to be added to it

### Progressive Development Protocol

- **Rule**: Start with simplest possible solutions and add complexity only after basic functionality is verified
- **Rationale**: Complex solutions built on unverified foundations often fail and require complete rework
- **Implementation**:
  - Test environment configuration before implementing complex features
  - Create minimal test scripts first, then enhance with advanced functionality
  - Start with simplest working solution, then enhance incrementally
  - Test each environment independently before assuming cross-compatibility

### External Data Validation Protocol

- **Rule**: Always verify external data source accessibility (URLs, APIs, files) before implementing data processing logic
- **Rationale**: External data sources can be unavailable or change format, causing downstream failures
- **Implementation**:
  - Test data sources at the HTTP level before implementing parsing and processing logic
  - Test CSV URLs, API endpoints, and data formats in isolation first
  - Have fallback strategies (e.g., sample data) when external sources are temporarily unavailable
- **Example**: CSV import fails → implement direct HTTP fetch to Google Sheets CSV URL

### Temporary File Management Protocol

- **Rule**: Always clean up temporary development files while preserving user-requested files
- **Rationale**: Temporary files clutter the workspace and can cause confusion, but user-requested files should be preserved
- **Implementation**:
  - Identify which files were created for debugging vs. user-requested functionality
  - Delete temporary investigation scripts after successful problem resolution
  - Preserve files that users specifically requested or that provide ongoing value
- **Example**: Delete `debug-*.js` and `investigate-*.js` files, keep `test-data-integrity.js` if user requested it

### Netlify Redirect Configuration Protocol

- **Rule**: When creating new API endpoints, always add corresponding redirects in netlify.toml to route requests appropriately
- **Rationale**: Frontend calls to new endpoints will fail without proper routing configuration. The system uses a hybrid approach: Next.js API routes for most endpoints, with Netlify redirects for external services (Heroku) and legacy Netlify Functions
- **Implementation**:
  - For Next.js API routes (app/api/*/route.ts): No redirect needed - Next.js handles routing automatically
  - For external services (Heroku debug/storage endpoints): Add redirect rules in netlify.toml pointing to external URLs
  - For legacy Netlify Functions: Add redirect rules pointing to `/.netlify/functions/function-name`
  - Test API endpoints directly before investigating frontend integration issues
  - Verify that frontend can successfully call new endpoints through the routing system
- **Example**: 
  - Next.js route: `app/api/chatbot/route.ts` → no redirect needed
  - External service: `/api/debug/*` → add redirect to `https://database-dorkinians-4bac3364a645.herokuapp.com/api/debug/:splat` in netlify.toml
  - Netlify Function: `/api/trigger-seed` → add redirect to `/.netlify/functions/trigger-seed` in netlify.toml

## Graph Database Operations

### Graph Data Safety Protocol

- **Rule**: When adding any nodes to the Neo4j graph, ensure the nodes have a property called "graphLabel" with the value "dorkiniansWebsite"
- **Rationale**: Prevents accidental deletion of data from other projects and ensures data isolation
- **Implementation**:
  - Always use the `createNode()` helper method which automatically adds `graphLabel` and `createdAt` properties
  - When clearing the graph, only delete nodes that have the property "graphLabel" with the value "dorkiniansWebsite"
  - Always use the `clearGraphData()` method which includes pre-deletion safety checks and logging
  - Implement safe deletion methods that verify `graphLabel` before any destructive operations

### Database Operations Protocol

- **Rule**: Use standardized helper methods for all Neo4j operations to ensure consistency and safety
- **Rationale**: Manual Cypher queries can introduce errors and bypass safety mechanisms
- **Implementation**:
  - Use the `createNode()` helper method which automatically adds `graphLabel` and `createdAt` properties
  - Use the `createRelationship()` helper method which ensures both nodes have the correct `graphLabel`
  - Query nodes using `getNodesByGraphLabel()` to ensure data isolation between projects
  - Use `getDatabaseStats()` to monitor only `dorkiniansWebsite` labeled data

### Schema Design Principles Protocol

- **Rule**: Prefer simple string properties over complex node types unless complex relationships provide clear analytical value
- **Rationale**: Complex graph relationships should only be created when they provide clear analytical value
- **Implementation**:
  - Only create unique constraints for fields that truly need to be unique across all records
  - Validate schema design assumptions against user requirements before implementation
  - Consider performance vs. complexity trade-offs in relationship design
- **Example**: Use simple string properties for team names rather than separate Team nodes unless team analysis is required

### Query Generation Safety Protocol

- **Rule**: Always validate and filter input properties before generating database queries, especially with dynamic schemas
- **Rationale**: Dynamic query construction can introduce security vulnerabilities and syntax errors
- **Implementation**:
  - Explicitly filter out system properties (like 'id') when building Neo4j SET clauses
  - Implement robust property filtering to prevent invalid Cypher syntax and ensure query safety
  - Test generated queries with sample data before execution to catch syntax errors early
- **Example**: Filter out system properties before building `SET n.property = value` clauses

